{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eb43cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from pydub import AudioSegment\n",
    "import warnings\n",
    "import collections\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# -----------------------------\n",
    "# Audio Processing Utilities\n",
    "# -----------------------------\n",
    "\n",
    "def convert_to_wav(file_path):\n",
    "    \"\"\"Convert non-WAV files to WAV using pydub.\"\"\"\n",
    "    try:\n",
    "        if file_path.lower().endswith('.wav'):\n",
    "            return file_path\n",
    "        wav_path = os.path.splitext(file_path)[0] + '.wav'\n",
    "        audio = AudioSegment.from_file(file_path)\n",
    "        audio.export(wav_path, format='wav')\n",
    "        return wav_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_features(audio, sample_rate):\n",
    "    \"\"\"Extract MFCC-based features along with additional spectral features.\"\"\"\n",
    "    try:\n",
    "        # MFCCs and their derivatives\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        delta = librosa.feature.delta(mfccs)\n",
    "        delta2 = librosa.feature.delta(mfccs, order=2)\n",
    "        \n",
    "        # Additional spectral features\n",
    "        spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sample_rate))\n",
    "        spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=audio, sr=sample_rate))\n",
    "        zero_crossing_rate = np.mean(librosa.feature.zero_crossing_rate(y=audio))\n",
    "        \n",
    "        features = np.hstack([\n",
    "            np.mean(mfccs.T, axis=0),\n",
    "            np.std(mfccs.T, axis=0),\n",
    "            np.max(mfccs.T, axis=0),\n",
    "            np.mean(delta.T, axis=0),\n",
    "            np.mean(delta2.T, axis=0),\n",
    "            spectral_centroid,\n",
    "            spectral_rolloff,\n",
    "            zero_crossing_rate\n",
    "        ])\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting features: {e}\")\n",
    "        return None\n",
    "\n",
    "def augment_audio(audio, sr):\n",
    "    \"\"\"Return list of augmented audio versions.\"\"\"\n",
    "    return [\n",
    "        librosa.effects.pitch_shift(audio, sr=sr, n_steps=random.uniform(-2, 2)),\n",
    "        librosa.effects.time_stretch(audio, rate=random.uniform(0.9, 1.1)),\n",
    "        audio + 0.005 * np.random.randn(len(audio))\n",
    "    ]\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset Preparation\n",
    "# -----------------------------\n",
    "\n",
    "def balance_dataset_with_augmentation(folder_names, base_path=\".\"):\n",
    "    \"\"\"\n",
    "    Load and balance dataset by augmenting minority classes\n",
    "    to match the size of the majority class.\n",
    "    \"\"\"\n",
    "    class_counts = {}\n",
    "    class_files = {}\n",
    "\n",
    "    for label, folder in enumerate(folder_names):\n",
    "        path = os.path.join(base_path, folder)\n",
    "        if not os.path.exists(path):\n",
    "            continue\n",
    "        files = [f for f in os.listdir(path) if f.lower().endswith(('.wav', '.mp3', '.m4a', '.ogg'))]\n",
    "        class_counts[label] = len(files)\n",
    "        class_files[label] = files\n",
    "\n",
    "    max_class = max(class_counts, key=class_counts.get)\n",
    "    max_samples = class_counts[max_class]\n",
    "    print(f\"\\nðŸ“Œ Max class: {folder_names[max_class]} with {max_samples} samples.\")\n",
    "\n",
    "    X, y = [], []\n",
    "\n",
    "    for label, files in class_files.items():\n",
    "        folder_path = os.path.join(base_path, folder_names[label])\n",
    "        current_features = []\n",
    "\n",
    "        for file in files:\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            wav_path = convert_to_wav(file_path)\n",
    "            if not wav_path:\n",
    "                continue\n",
    "            try:\n",
    "                audio, sr = librosa.load(wav_path, res_type='kaiser_fast')\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {wav_path}: {e}\")\n",
    "                continue\n",
    "            features = extract_features(audio, sr)\n",
    "            if features is not None:\n",
    "                current_features.append(features)\n",
    "\n",
    "        X.extend(current_features)\n",
    "        y.extend([label] * len(current_features))\n",
    "\n",
    "        # Augment data for minority classes\n",
    "        if label == max_class:\n",
    "            continue\n",
    "\n",
    "        current_count = len(current_features)\n",
    "        while current_count < max_samples:\n",
    "            for file in files:\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                wav_path = convert_to_wav(file_path)\n",
    "                if not wav_path:\n",
    "                    continue\n",
    "                try:\n",
    "                    audio, sr = librosa.load(wav_path, res_type='kaiser_fast')\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {wav_path}: {e}\")\n",
    "                    continue\n",
    "                for aug_audio in augment_audio(audio, sr):\n",
    "                    aug_features = extract_features(aug_audio, sr)\n",
    "                    if aug_features is not None:\n",
    "                        X.append(aug_features)\n",
    "                        y.append(label)\n",
    "                        current_count += 1\n",
    "                    if current_count >= max_samples:\n",
    "                        break\n",
    "                if current_count >= max_samples:\n",
    "                    break\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# -----------------------------\n",
    "# Model Training\n",
    "# -----------------------------\n",
    "\n",
    "def train_model(model, X_train, X_test, y_train, y_test, scaler=None):\n",
    "    \"\"\"Generic model trainer and evaluator.\"\"\"\n",
    "    if scaler:\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    return model, acc, scaler\n",
    "\n",
    "# -----------------------------\n",
    "# Main Pipeline\n",
    "# -----------------------------\n",
    "\n",
    "def main():\n",
    "    # No fixed seeds here to allow variability in each run\n",
    "    folder_names = ['belly_pain', 'burping', 'discomfort', 'hungry', 'tired']\n",
    "    print(\"ðŸ“¦ Preparing balanced dataset with augmentation...\")\n",
    "    X, y = balance_dataset_with_augmentation(folder_names)\n",
    "\n",
    "    # Show class distribution\n",
    "    label_counts = collections.Counter(y)\n",
    "    print(\"\\nðŸ“Š Class distribution after augmentation:\")\n",
    "    for label, count in sorted(label_counts.items()):\n",
    "        print(f\"{folder_names[label]}: {count}\")\n",
    "\n",
    "    # Split the dataset without a fixed random state\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y\n",
    "    )\n",
    "    print(f\"\\nðŸ”¹ Train size: {len(X_train)}, Test size: {len(X_test)}\")\n",
    "    print(f\"ðŸ”¹ Feature size: {X.shape[1]}\")\n",
    "\n",
    "    # SVM is tuned using GridSearchCV with a small parameter grid.\n",
    "    svm_model = SVC(kernel='rbf')\n",
    "    param_grid = {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto']}\n",
    "    svm_grid = GridSearchCV(svm_model, param_grid, cv=3)\n",
    "    \n",
    "    models = {\n",
    "        'Random Forest': (RandomForestClassifier(), None),\n",
    "        'XGBoost': (XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='mlogloss'), None),\n",
    "        'SVM (RBF)': (svm_grid, StandardScaler()),\n",
    "        'Logistic Regression': (LogisticRegression(max_iter=1000), StandardScaler()),\n",
    "        'k-NN': (KNeighborsClassifier(n_neighbors=5), StandardScaler()),\n",
    "        'Gradient Boosting': (GradientBoostingClassifier(), None),\n",
    "        'Naive Bayes': (GaussianNB(), None),\n",
    "        'MLP': (MLPClassifier(hidden_layer_sizes=(100,), max_iter=500), StandardScaler())\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    trained_models = {}\n",
    "\n",
    "    for name, (model, scaler) in models.items():\n",
    "        print(f\"\\nðŸš€ Training {name}...\")\n",
    "        trained_model, acc, trained_scaler = train_model(model, X_train, X_test, y_train, y_test, scaler)\n",
    "        results[name] = acc\n",
    "        trained_models[name] = (trained_model, trained_scaler)\n",
    "        print(f\"{name} Accuracy: {acc * 100:.2f}%\")\n",
    "\n",
    "    print(\"\\nðŸ“ˆ Final Model Accuracies (sorted):\")\n",
    "    for name, acc in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{name}: {acc * 100:.2f}%\")\n",
    "\n",
    "    def predict_audio(file_path, model, scaler=None):\n",
    "        wav_path = convert_to_wav(file_path)\n",
    "        if not wav_path:\n",
    "            return \"Invalid audio\"\n",
    "        audio, sr = librosa.load(wav_path, res_type='kaiser_fast')\n",
    "        features = extract_features(audio, sr)\n",
    "        if features is None:\n",
    "            return \"Could not extract features\"\n",
    "        features = [features]\n",
    "        if scaler:\n",
    "            features = scaler.transform(features)\n",
    "        pred = model.predict(features)[0]\n",
    "        return folder_names[pred]\n",
    "\n",
    "    return results, trained_models, predict_audio\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e07f8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Preparing balanced dataset with augmentation...\n",
      "\n",
      "ðŸ“Œ Max class: hungry with 382 samples.\n",
      "\n",
      "ðŸ“Š Class distribution after augmentation:\n",
      "belly_pain: 382\n",
      "burping: 382\n",
      "discomfort: 382\n",
      "hungry: 382\n",
      "tired: 382\n",
      "\n",
      "ðŸ”¹ Train size: 1528, Test size: 382\n",
      "ðŸ”¹ Feature size: 203\n",
      "\n",
      "ðŸš€ Training Random Forest...\n",
      "Random Forest Accuracy: 99.21%\n",
      "\n",
      "ðŸš€ Training XGBoost...\n",
      "XGBoost Accuracy: 97.64%\n",
      "\n",
      "ðŸš€ Training SVM (RBF)...\n",
      "SVM (RBF) Accuracy: 99.48%\n",
      "\n",
      "ðŸš€ Training Logistic Regression...\n",
      "Logistic Regression Accuracy: 92.67%\n",
      "\n",
      "ðŸš€ Training k-NN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"d:\\defence\\model\\donateacry_corpus_cleaned_and_updated_data\\.conda\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "  File \"d:\\defence\\model\\donateacry_corpus_cleaned_and_updated_data\\.conda\\lib\\subprocess.py\", line 503, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "  File \"d:\\defence\\model\\donateacry_corpus_cleaned_and_updated_data\\.conda\\lib\\subprocess.py\", line 971, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"d:\\defence\\model\\donateacry_corpus_cleaned_and_updated_data\\.conda\\lib\\subprocess.py\", line 1456, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-NN Accuracy: 89.53%\n",
      "\n",
      "ðŸš€ Training Gradient Boosting...\n",
      "Gradient Boosting Accuracy: 95.81%\n",
      "\n",
      "ðŸš€ Training Naive Bayes...\n",
      "Naive Bayes Accuracy: 62.83%\n",
      "\n",
      "ðŸš€ Training MLP...\n",
      "MLP Accuracy: 97.64%\n",
      "\n",
      "ðŸ“ˆ Final Model Accuracies (sorted):\n",
      "SVM (RBF): 99.48%\n",
      "Random Forest: 99.21%\n",
      "XGBoost: 97.64%\n",
      "MLP: 97.64%\n",
      "Gradient Boosting: 95.81%\n",
      "Logistic Regression: 92.67%\n",
      "k-NN: 89.53%\n",
      "Naive Bayes: 62.83%\n",
      "ðŸ“¦ Preparing balanced dataset with augmentation...\n",
      "\n",
      "ðŸ“Œ Max class: hungry with 382 samples.\n",
      "\n",
      "ðŸ“Š Class distribution after augmentation:\n",
      "belly_pain: 382\n",
      "burping: 382\n",
      "discomfort: 382\n",
      "hungry: 382\n",
      "tired: 382\n",
      "\n",
      "ðŸ”¹ Train size: 1528, Test size: 382\n",
      "ðŸ”¹ Feature size: 203\n",
      "\n",
      "ðŸš€ Training Random Forest...\n",
      "Random Forest Accuracy: 97.91%\n",
      "\n",
      "ðŸš€ Training XGBoost...\n",
      "XGBoost Accuracy: 96.60%\n",
      "\n",
      "ðŸš€ Training SVM (RBF)...\n",
      "SVM (RBF) Accuracy: 97.64%\n",
      "\n",
      "ðŸš€ Training Logistic Regression...\n",
      "Logistic Regression Accuracy: 93.19%\n",
      "\n",
      "ðŸš€ Training k-NN...\n",
      "k-NN Accuracy: 87.43%\n",
      "\n",
      "ðŸš€ Training Gradient Boosting...\n",
      "Gradient Boosting Accuracy: 95.03%\n",
      "\n",
      "ðŸš€ Training Naive Bayes...\n",
      "Naive Bayes Accuracy: 63.09%\n",
      "\n",
      "ðŸš€ Training MLP...\n",
      "MLP Accuracy: 96.07%\n",
      "\n",
      "ðŸ“ˆ Final Model Accuracies (sorted):\n",
      "Random Forest: 97.91%\n",
      "SVM (RBF): 97.64%\n",
      "XGBoost: 96.60%\n",
      "MLP: 96.07%\n",
      "Gradient Boosting: 95.03%\n",
      "Logistic Regression: 93.19%\n",
      "k-NN: 87.43%\n",
      "Naive Bayes: 63.09%\n",
      "ðŸ“¦ Preparing balanced dataset with augmentation...\n",
      "\n",
      "ðŸ“Œ Max class: hungry with 382 samples.\n",
      "\n",
      "ðŸ“Š Class distribution after augmentation:\n",
      "belly_pain: 382\n",
      "burping: 382\n",
      "discomfort: 382\n",
      "hungry: 382\n",
      "tired: 382\n",
      "\n",
      "ðŸ”¹ Train size: 1528, Test size: 382\n",
      "ðŸ”¹ Feature size: 203\n",
      "\n",
      "ðŸš€ Training Random Forest...\n",
      "Random Forest Accuracy: 97.38%\n",
      "\n",
      "ðŸš€ Training XGBoost...\n",
      "XGBoost Accuracy: 95.29%\n",
      "\n",
      "ðŸš€ Training SVM (RBF)...\n",
      "SVM (RBF) Accuracy: 97.38%\n",
      "\n",
      "ðŸš€ Training Logistic Regression...\n",
      "Logistic Regression Accuracy: 89.01%\n",
      "\n",
      "ðŸš€ Training k-NN...\n",
      "k-NN Accuracy: 86.91%\n",
      "\n",
      "ðŸš€ Training Gradient Boosting...\n",
      "Gradient Boosting Accuracy: 95.29%\n",
      "\n",
      "ðŸš€ Training Naive Bayes...\n",
      "Naive Bayes Accuracy: 56.81%\n",
      "\n",
      "ðŸš€ Training MLP...\n",
      "MLP Accuracy: 93.98%\n",
      "\n",
      "ðŸ“ˆ Final Model Accuracies (sorted):\n",
      "Random Forest: 97.38%\n",
      "SVM (RBF): 97.38%\n",
      "XGBoost: 95.29%\n",
      "Gradient Boosting: 95.29%\n",
      "MLP: 93.98%\n",
      "Logistic Regression: 89.01%\n",
      "k-NN: 86.91%\n",
      "Naive Bayes: 56.81%\n",
      "ðŸ“¦ Preparing balanced dataset with augmentation...\n",
      "\n",
      "ðŸ“Œ Max class: hungry with 382 samples.\n",
      "\n",
      "ðŸ“Š Class distribution after augmentation:\n",
      "belly_pain: 382\n",
      "burping: 382\n",
      "discomfort: 382\n",
      "hungry: 382\n",
      "tired: 382\n",
      "\n",
      "ðŸ”¹ Train size: 1528, Test size: 382\n",
      "ðŸ”¹ Feature size: 203\n",
      "\n",
      "ðŸš€ Training Random Forest...\n",
      "Random Forest Accuracy: 98.17%\n",
      "\n",
      "ðŸš€ Training XGBoost...\n",
      "XGBoost Accuracy: 97.64%\n",
      "\n",
      "ðŸš€ Training SVM (RBF)...\n",
      "SVM (RBF) Accuracy: 98.43%\n",
      "\n",
      "ðŸš€ Training Logistic Regression...\n",
      "Logistic Regression Accuracy: 91.62%\n",
      "\n",
      "ðŸš€ Training k-NN...\n",
      "k-NN Accuracy: 86.39%\n",
      "\n",
      "ðŸš€ Training Gradient Boosting...\n",
      "Gradient Boosting Accuracy: 97.12%\n",
      "\n",
      "ðŸš€ Training Naive Bayes...\n",
      "Naive Bayes Accuracy: 62.30%\n",
      "\n",
      "ðŸš€ Training MLP...\n",
      "MLP Accuracy: 95.03%\n",
      "\n",
      "ðŸ“ˆ Final Model Accuracies (sorted):\n",
      "SVM (RBF): 98.43%\n",
      "Random Forest: 98.17%\n",
      "XGBoost: 97.64%\n",
      "Gradient Boosting: 97.12%\n",
      "MLP: 95.03%\n",
      "Logistic Regression: 91.62%\n",
      "k-NN: 86.39%\n",
      "Naive Bayes: 62.30%\n",
      "ðŸ“¦ Preparing balanced dataset with augmentation...\n",
      "\n",
      "ðŸ“Œ Max class: hungry with 382 samples.\n",
      "\n",
      "ðŸ“Š Class distribution after augmentation:\n",
      "belly_pain: 382\n",
      "burping: 382\n",
      "discomfort: 382\n",
      "hungry: 382\n",
      "tired: 382\n",
      "\n",
      "ðŸ”¹ Train size: 1528, Test size: 382\n",
      "ðŸ”¹ Feature size: 203\n",
      "\n",
      "ðŸš€ Training Random Forest...\n",
      "Random Forest Accuracy: 96.86%\n",
      "\n",
      "ðŸš€ Training XGBoost...\n",
      "XGBoost Accuracy: 96.60%\n",
      "\n",
      "ðŸš€ Training SVM (RBF)...\n",
      "SVM (RBF) Accuracy: 98.95%\n",
      "\n",
      "ðŸš€ Training Logistic Regression...\n",
      "Logistic Regression Accuracy: 90.31%\n",
      "\n",
      "ðŸš€ Training k-NN...\n",
      "k-NN Accuracy: 90.31%\n",
      "\n",
      "ðŸš€ Training Gradient Boosting...\n",
      "Gradient Boosting Accuracy: 93.46%\n",
      "\n",
      "ðŸš€ Training Naive Bayes...\n",
      "Naive Bayes Accuracy: 57.59%\n",
      "\n",
      "ðŸš€ Training MLP...\n",
      "MLP Accuracy: 96.60%\n",
      "\n",
      "ðŸ“ˆ Final Model Accuracies (sorted):\n",
      "SVM (RBF): 98.95%\n",
      "Random Forest: 96.86%\n",
      "XGBoost: 96.60%\n",
      "MLP: 96.60%\n",
      "Gradient Boosting: 93.46%\n",
      "Logistic Regression: 90.31%\n",
      "k-NN: 90.31%\n",
      "Naive Bayes: 57.59%\n",
      "ðŸ“¦ Preparing balanced dataset with augmentation...\n",
      "\n",
      "ðŸ“Œ Max class: hungry with 382 samples.\n",
      "\n",
      "ðŸ“Š Class distribution after augmentation:\n",
      "belly_pain: 382\n",
      "burping: 382\n",
      "discomfort: 382\n",
      "hungry: 382\n",
      "tired: 382\n",
      "\n",
      "ðŸ”¹ Train size: 1528, Test size: 382\n",
      "ðŸ”¹ Feature size: 203\n",
      "\n",
      "ðŸš€ Training Random Forest...\n",
      "Random Forest Accuracy: 97.38%\n",
      "\n",
      "ðŸš€ Training XGBoost...\n",
      "XGBoost Accuracy: 96.60%\n",
      "\n",
      "ðŸš€ Training SVM (RBF)...\n",
      "SVM (RBF) Accuracy: 98.43%\n",
      "\n",
      "ðŸš€ Training Logistic Regression...\n",
      "Logistic Regression Accuracy: 89.53%\n",
      "\n",
      "ðŸš€ Training k-NN...\n",
      "k-NN Accuracy: 85.86%\n",
      "\n",
      "ðŸš€ Training Gradient Boosting...\n",
      "Gradient Boosting Accuracy: 97.91%\n",
      "\n",
      "ðŸš€ Training Naive Bayes...\n",
      "Naive Bayes Accuracy: 62.57%\n",
      "\n",
      "ðŸš€ Training MLP...\n",
      "MLP Accuracy: 95.29%\n",
      "\n",
      "ðŸ“ˆ Final Model Accuracies (sorted):\n",
      "SVM (RBF): 98.43%\n",
      "Gradient Boosting: 97.91%\n",
      "Random Forest: 97.38%\n",
      "XGBoost: 96.60%\n",
      "MLP: 95.29%\n",
      "Logistic Regression: 89.53%\n",
      "k-NN: 85.86%\n",
      "Naive Bayes: 62.57%\n",
      "ðŸ“¦ Preparing balanced dataset with augmentation...\n",
      "\n",
      "ðŸ“Œ Max class: hungry with 382 samples.\n",
      "\n",
      "ðŸ“Š Class distribution after augmentation:\n",
      "belly_pain: 382\n",
      "burping: 382\n",
      "discomfort: 382\n",
      "hungry: 382\n",
      "tired: 382\n",
      "\n",
      "ðŸ”¹ Train size: 1528, Test size: 382\n",
      "ðŸ”¹ Feature size: 203\n",
      "\n",
      "ðŸš€ Training Random Forest...\n",
      "Random Forest Accuracy: 97.91%\n",
      "\n",
      "ðŸš€ Training XGBoost...\n",
      "XGBoost Accuracy: 97.12%\n",
      "\n",
      "ðŸš€ Training SVM (RBF)...\n",
      "SVM (RBF) Accuracy: 98.95%\n",
      "\n",
      "ðŸš€ Training Logistic Regression...\n",
      "Logistic Regression Accuracy: 92.93%\n",
      "\n",
      "ðŸš€ Training k-NN...\n",
      "k-NN Accuracy: 87.43%\n",
      "\n",
      "ðŸš€ Training Gradient Boosting...\n",
      "Gradient Boosting Accuracy: 96.60%\n",
      "\n",
      "ðŸš€ Training Naive Bayes...\n",
      "Naive Bayes Accuracy: 62.57%\n",
      "\n",
      "ðŸš€ Training MLP...\n",
      "MLP Accuracy: 95.55%\n",
      "\n",
      "ðŸ“ˆ Final Model Accuracies (sorted):\n",
      "SVM (RBF): 98.95%\n",
      "Random Forest: 97.91%\n",
      "XGBoost: 97.12%\n",
      "Gradient Boosting: 96.60%\n",
      "MLP: 95.55%\n",
      "Logistic Regression: 92.93%\n",
      "k-NN: 87.43%\n",
      "Naive Bayes: 62.57%\n",
      "ðŸ“¦ Preparing balanced dataset with augmentation...\n",
      "\n",
      "ðŸ“Œ Max class: hungry with 382 samples.\n",
      "\n",
      "ðŸ“Š Class distribution after augmentation:\n",
      "belly_pain: 382\n",
      "burping: 382\n",
      "discomfort: 382\n",
      "hungry: 382\n",
      "tired: 382\n",
      "\n",
      "ðŸ”¹ Train size: 1528, Test size: 382\n",
      "ðŸ”¹ Feature size: 203\n",
      "\n",
      "ðŸš€ Training Random Forest...\n",
      "Random Forest Accuracy: 97.64%\n",
      "\n",
      "ðŸš€ Training XGBoost...\n",
      "XGBoost Accuracy: 97.91%\n",
      "\n",
      "ðŸš€ Training SVM (RBF)...\n",
      "SVM (RBF) Accuracy: 98.95%\n",
      "\n",
      "ðŸš€ Training Logistic Regression...\n",
      "Logistic Regression Accuracy: 92.15%\n",
      "\n",
      "ðŸš€ Training k-NN...\n",
      "k-NN Accuracy: 86.39%\n",
      "\n",
      "ðŸš€ Training Gradient Boosting...\n",
      "Gradient Boosting Accuracy: 95.29%\n",
      "\n",
      "ðŸš€ Training Naive Bayes...\n",
      "Naive Bayes Accuracy: 59.69%\n",
      "\n",
      "ðŸš€ Training MLP...\n",
      "MLP Accuracy: 95.55%\n",
      "\n",
      "ðŸ“ˆ Final Model Accuracies (sorted):\n",
      "SVM (RBF): 98.95%\n",
      "XGBoost: 97.91%\n",
      "Random Forest: 97.64%\n",
      "MLP: 95.55%\n",
      "Gradient Boosting: 95.29%\n",
      "Logistic Regression: 92.15%\n",
      "k-NN: 86.39%\n",
      "Naive Bayes: 59.69%\n",
      "ðŸ“¦ Preparing balanced dataset with augmentation...\n",
      "\n",
      "ðŸ“Œ Max class: hungry with 382 samples.\n",
      "\n",
      "ðŸ“Š Class distribution after augmentation:\n",
      "belly_pain: 382\n",
      "burping: 382\n",
      "discomfort: 382\n",
      "hungry: 382\n",
      "tired: 382\n",
      "\n",
      "ðŸ”¹ Train size: 1528, Test size: 382\n",
      "ðŸ”¹ Feature size: 203\n",
      "\n",
      "ðŸš€ Training Random Forest...\n",
      "Random Forest Accuracy: 97.91%\n",
      "\n",
      "ðŸš€ Training XGBoost...\n",
      "XGBoost Accuracy: 95.03%\n",
      "\n",
      "ðŸš€ Training SVM (RBF)...\n",
      "SVM (RBF) Accuracy: 97.91%\n",
      "\n",
      "ðŸš€ Training Logistic Regression...\n",
      "Logistic Regression Accuracy: 90.84%\n",
      "\n",
      "ðŸš€ Training k-NN...\n",
      "k-NN Accuracy: 86.91%\n",
      "\n",
      "ðŸš€ Training Gradient Boosting...\n",
      "Gradient Boosting Accuracy: 95.03%\n",
      "\n",
      "ðŸš€ Training Naive Bayes...\n",
      "Naive Bayes Accuracy: 60.21%\n",
      "\n",
      "ðŸš€ Training MLP...\n",
      "MLP Accuracy: 95.55%\n",
      "\n",
      "ðŸ“ˆ Final Model Accuracies (sorted):\n",
      "Random Forest: 97.91%\n",
      "SVM (RBF): 97.91%\n",
      "MLP: 95.55%\n",
      "XGBoost: 95.03%\n",
      "Gradient Boosting: 95.03%\n",
      "Logistic Regression: 90.84%\n",
      "k-NN: 86.91%\n",
      "Naive Bayes: 60.21%\n",
      "ðŸ“¦ Preparing balanced dataset with augmentation...\n",
      "\n",
      "ðŸ“Œ Max class: hungry with 382 samples.\n",
      "\n",
      "ðŸ“Š Class distribution after augmentation:\n",
      "belly_pain: 382\n",
      "burping: 382\n",
      "discomfort: 382\n",
      "hungry: 382\n",
      "tired: 382\n",
      "\n",
      "ðŸ”¹ Train size: 1528, Test size: 382\n",
      "ðŸ”¹ Feature size: 203\n",
      "\n",
      "ðŸš€ Training Random Forest...\n",
      "Random Forest Accuracy: 97.64%\n",
      "\n",
      "ðŸš€ Training XGBoost...\n",
      "XGBoost Accuracy: 96.34%\n",
      "\n",
      "ðŸš€ Training SVM (RBF)...\n",
      "SVM (RBF) Accuracy: 97.91%\n",
      "\n",
      "ðŸš€ Training Logistic Regression...\n",
      "Logistic Regression Accuracy: 90.84%\n",
      "\n",
      "ðŸš€ Training k-NN...\n",
      "k-NN Accuracy: 88.22%\n",
      "\n",
      "ðŸš€ Training Gradient Boosting...\n",
      "Gradient Boosting Accuracy: 96.86%\n",
      "\n",
      "ðŸš€ Training Naive Bayes...\n",
      "Naive Bayes Accuracy: 56.02%\n",
      "\n",
      "ðŸš€ Training MLP...\n",
      "MLP Accuracy: 94.50%\n",
      "\n",
      "ðŸ“ˆ Final Model Accuracies (sorted):\n",
      "SVM (RBF): 97.91%\n",
      "Random Forest: 97.64%\n",
      "Gradient Boosting: 96.86%\n",
      "XGBoost: 96.34%\n",
      "MLP: 94.50%\n",
      "Logistic Regression: 90.84%\n",
      "k-NN: 88.22%\n",
      "Naive Bayes: 56.02%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_models = []\n",
    "avg_acc = 0.0\n",
    "runtimes = 10  # Adjust the number of runs as needed\n",
    "for i in range(runtimes):\n",
    "        results, trained_models, predict_func = main()\n",
    "        # Find the best model name and its accuracy for this run\n",
    "        best_model_name, best_accuracy = max(results.items(), key=lambda x: x[1])\n",
    "        best_models.append([best_model_name, best_accuracy])\n",
    "        avg_acc += best_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab1d59b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'best_models.csv' has been saved\n",
      "Average Accuracy = 0.9842931937172773\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "    # Create a pandas DataFrame from best_models list with appropriate columns\n",
    "df = pd.DataFrame(best_models, columns=[\"Model\", \"Accuracy\"])\n",
    "df[\"Run\"] = range(1, len(best_models) + 1)\n",
    "    # Reorder columns\n",
    "df = df[[\"Run\", \"Model\", \"Accuracy\"]]\n",
    "    \n",
    "    # Save DataFrame to a CSV file\n",
    "df.to_csv(\"best_models.csv\", index=False)\n",
    "    \n",
    "print(\"CSV file 'best_models.csv' has been saved\")\n",
    "\n",
    "print(f'Average Accuracy = {avg_acc / runtimes}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
